<!-------------------------------------------------------------------------------------
*
* MIT License
* Copyright (c) 2020 Raghuveer S
*
* Permission is hereby granted, free of charge, to any person obtaining a copy
* of this software and associated documentation files (the "Software"), to deal
* in the Software without restriction, including without limitation the rights
* to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
* copies of the Software, and to permit persons to whom the Software is
* furnished to do so, subject to the following conditions:
*
* The above copyright notice and this permission notice shall be included in all
* copies or substantial portions of the Software.
*
* THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
* IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
* FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
* AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
* LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
* OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
* SOFTWARE.
*
------------------------------------------------------------------------------------>
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta content="width=device-width, initial-scale=1" name="viewport" />
    <meta content="Knowledge Continuum" property="og:site_name"/>
    <meta content="A non-linear thread of important ideas weaved together to explore and understand the fabric of knowledge." property="og:description">
    <meta content="https://itskd.github.io/about/" property="article:author"><meta content="Colorization - Sampling Color from GrayScale and I.R. Images" property="og:title">
    <meta content="article" property="og:type">
    <meta content="https://itskd.github.io/notes/study-of-colorization" property="og:url"><title>Notes & Thoughts</title>
    <link rel="canonical" href="https://itskd.github.io/notes/study-of-colorization"/>
    <link rel="apple-touch-icon" href="/assets/img/profile.png">
    <link rel="icon" href="/assets/img/favicon.png" type="image/png" sizes="16x16"/>
    <link href="/assets/css/Style.css" rel="stylesheet" media="all" class="default" />
    <link href="/assets/css/Util.css" rel="stylesheet" media="all" class="default" />
    <link href="/assets/css/vendor/Katex.css" rel="stylesheet" media="all" class="default"/>
    <!--[if IE]>
      <link href="/assets/css/ie-target.css" rel="stylesheet" type="text/css"/>
    <![endif]-->
    <!--<link href="/assets/css/prism.css" rel="stylesheet" />-->
    <link rel="alternate" type="application/rss+xml" href="https://itskd.github.io/feed.xml">
  </head>
  <body>
    <div class="container">
      <div class = "box">
        <header>
          <div class="dashboard disable-select">
            <div class="site-heading-ltr profile-board-col">
              <p class="main-page-heading medium margin-top-30"><a href='/'>Notes & Thoughts</a></p>
            </div>
          </div>
          <div class="main-site-subheader menu disable-select">
            <a class="menu-item" href="/about">
              <svg class="menu-item-icon" width="18" height="19" viewBox="0 0 25 25" fill="none"
                    xmlns="http://www.w3.org/2000/svg">
                <path
                        d="M20.9777 21.6138V19.6138C20.9777 18.553 20.5563 17.5356 19.8061 16.7854C19.056 16.0353 18.0386 15.6138 16.9777 15.6138H8.97768C7.91682 15.6138 6.8994 16.0353 6.14926 16.7854C5.39911 17.5356 4.97768 18.553 4.97768 19.6138V21.6138"
                        stroke-width="2" stroke-linecap="round" stroke-linejoin="round" />
                  <path
                        d="M12.9777 11.6138C15.1868 11.6138 16.9777 9.82298 16.9777 7.61385C16.9777 5.40471 15.1868 3.61385 12.9777 3.61385C10.7685 3.61385 8.97768 5.40471 8.97768 7.61385C8.97768 9.82298 10.7685 11.6138 12.9777 11.6138Z"
                        stroke-width="2" stroke-linecap="round" stroke-linejoin="round" />
                  </svg>
                  <p class="menu-item-text">About</p>
                </a>
                <a class="menu-item" href="/tags">
                  <svg class="menu-item-icon" width="18" height="19" viewBox="0 0 24 25" fill="none"
                    xmlns="http://www.w3.org/2000/svg">
                    <path d="M4 9.5H20" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" />
                    <path d="M4 15.5H20" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" />
                    <path d="M10 3.5L8 21.5" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" />
                    <path d="M16 3.5L14 21.5" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" />
                  </svg>
                  <p class="menu-item-text">Tags</p>
                </a>
                <a class="menu-item" href="/feed.xml">
                  <svg class="menu-item-icon" width="18" height="19" viewBox="0 0 24 25" fill="none" xmlns="http://www.w3.org/2000/svg">
                    <path d="M4 11.5C6.38695 11.5 8.67613 12.4482 10.364 14.136C12.0518 15.8239 13 18.1131 13 20.5"
                        stroke-width="2" stroke-linecap="round" stroke-linejoin="round" />
                    <path d="M4 4.5C8.24346 4.5 12.3131 6.18571 15.3137 9.18629C18.3143 12.1869 20 16.2565 20 20.5"
                        stroke-width="2" stroke-linecap="round" stroke-linejoin="round" />
                    <path
                        d="M5 20.5C5.55228 20.5 6 20.0523 6 19.5C6 18.9477 5.55228 18.5 5 18.5C4.44772 18.5 4 18.9477 4 19.5C4 20.0523 4.44772 20.5 5 20.5Z"
                        stroke-width="2" stroke-linecap="round" stroke-linejoin="round" />
                    </svg>
                    <p class="menu-item-text">RSS</p>
                  </a>
                </div>
                <div class="searchbar search-container">
                  <svg class="search-icon" width="19" height="19" viewBox="0 0 25 25" fill="none" xmlns="http://www.w3.org/2000/svg">
                    <path d="M11.5 19.5C15.9183 19.5 19.5 15.9183 19.5 11.5C19.5 7.08172 15.9183 3.5 11.5 3.5C7.08172 3.5 3.5 7.08172 3.5 11.5C3.5 15.9183 7.08172 19.5 11.5 19.5Z" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                    <path d="M21.5 21.5L17.15 17.15"  stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                  </svg>
                  <div class="search-shortcut disable-select">
                    <kbd class="disable-select">Shift</kbd>
                    <kbd class="disable-select">s</kbd>
                  </div>
                  <label for="search-input"></label>
                  <input type="text" id="search-input" autocomplete="off" placeholder="Search Notes & Thoughts..."/>
                  <div id="search-results" class="search-results"></div>
                </div>
                <script type="text/javascript" src="/assets/js/vendor/lunr.min.js"></script>
                <script src="/assets/js/Search.js"></script>
                <div class="main-site-subheader disable-select" id="scroll-head">
                  <div class="back-icon" onclick="window.location.assign('/');">
                    <svg class="ripple" xmlns="http://www.w3.org/2000/svg" height="24" viewBox="0 0 24 24" width="24">
                      <path d="M0 0h24v24H0z" fill="none"/>
                      <path d="M21 11H6.83l3.58-3.59L9 6l-6 6 6 6 1.41-1.41L6.83 13H21z"/>
                    </svg>
                  </div>
                  <p class="back-p">Home</p>
                </div>
              </header>
              <main>
                <h1>Colorization - Sampling Color from GrayScale and I.R. Images</h1>
                <!-- Parse internal links, external links, transclusions etc and manipuate the content to reflect it accordingly -->
                <ul class="tags">
                  <li><a href="/dates/#14-January-2021" class="tag">January-14-2021</a></li>
                  <!-- Loop through page categories and print them in tags -->
                  <li><a href="/tags/#notes" class="tag">notes</a></li>
                  <li><a href="/tags/#colorization" class="tag">colorization</a></li>
                  <li><a href="/tags/#main" class="tag">main</a></li>
                </ul>
                <div class="content">
                  <p>Given a grayscale image, it is a daunting task, even for a human, to visualize it in color. See Figure 1 for examples. However, a human may try to find semantic clues like texture and world knowledge to assign colors to objects. For example, the grass is mostly green, or the sky is mostly blue. But these clues may also fail sometimes, as shown in Figure 1(middle). Thus, in this work, the focus was on assigning a plausible set of colors to the Image, which may or may not be the same as the ground truth.</p>
                  <p><img src="../assets/imgs/2021-01-21-15-43-47.png" alt="" /></p>
                  <p><em>Fig:1 GrayScale Images and Corresponding Color</em></p>
                  <blockquote>
                    <p>The primary motivation behind pursuing this problem was that many images do not have color information. Also, the problem of Colorization is self-supervised and does not require a pair-wise dataset.</p>
                  </blockquote>
                  <p>The aim is to solve it in generative fashion, such that if we feed the same grayscale Image to the network k times, it may generate different output each time. A generative network’s benefit is that it may color the cloth’s stripes (Figure 2), gray or red.</p>
                  <p><img src="../assets/imgs/2021-01-22-11-34-00.png" alt="" /></p>
                  <p><em>Fig:2 Plants in the left Image are entirely green, not in the right</em></p>
                  <p>The solution is based on PixColor, which is state of the art autoregressive generative neural networks for Colorization, i.e., the output of $i^{th}$ pixel is not just conditioned on the latent representation of the grayscale Image, X but also on the previous outputs, $[i-t, i-t+1, i-t+2\ …\ i-1]$ where t denotes the receptive field.</p>
                  <p>Given $X \in [H, W]$, we first extracts the features $Y_1$ using Resnet-101 of size $[\frac{H}{4}, \frac{W}{4}, 1024]$. These features are then passed into an adaption network and use three convolution layers to adapt the features required by pixelcnn. The output from the adaption network is of size $[\frac{H}{4}, \frac{W}{4}, 64]$, and is fed into conditional pixelcnn. It masks the weights of a convolutional layer to prohibit pixel $x_i$ from using any information about the future samples $(x_{i+1:N})$.</p>
                  <p>Training is the same as that of any other end-to-end trainable architecture (as ground truth data was used under teacher training mechanism), but during testing, for each pixel $i$, the class is sampled from a multinomial distribution defined by the softmax output of the network.</p>
                  <p><img src="../assets/imgs/2021-01-22-15-36-30.png" alt="" />
                    <em>Fig:3 Result of Colorization Algorithm</em></p>
                  <p>The first seven images in carousal(at the top) are the PixColor algorithm results with key insights from each of the Images.</p>
                  <h2 id="extension-towards-ir-images">Extension towards I.R. images</h2>
                  <p>I decided to continue working on image colorization during my next semester, focusing on ​reducing the artifacts and ​improving larger objects’ coloring​. The output from pixelcnn is given to a fully convolutional network, acting as a denoiser, inspired by Tacotron, a source synthesis architecture.
                    <img src="../assets/imgs/2021-01-22-15-16-49.png" alt="" />
                    <em>Fig:4 Correction of Green Artifact as shown in left Image</em></p>
                  <p>After that, my professor suggested applying image colorization on I.R. images. In applications under low lighting, I.R. cameras come in handy, but interpreting I.R. images is not straightforward for a human, and hence translating to RGB improves its understandability. I.R. images introduced two challenges, i) it is no longer a self-supervised task and requires a parallel dataset, ii) it is computationally expensive since with grayscale images, we can learn the color information at less spatial resolution(Figure 5-middle) and upscale it, with minimal impact on visual quality but with I.R. images, we need to learn Luminicance too(Figure 5-bottom).</p>
                  <table>
                    <thead>
                      <tr>
                        <th>Input Image</th>
                        <th><img src="../assets/imgs/2021-01-22-15-22-11.png" alt="" /></th>
                      </tr>
                    </thead>
                    <tbody>
                      <tr>
                        <td>Color channels<br />
                           downscaled and<br />
                           Interploated</td>
                        <td><img src="../assets/imgs/2021-01-22-15-22-58.png" alt="" /></td>
                      </tr>
                      <tr>
                        <td>All channels are<br />
                           downscaled and<br />
                           interpolated</td>
                        <td><img src="../assets/imgs/2021-01-22-15-23-21.png" alt="" /></td>
                      </tr>
                    </tbody>
                  </table>
                  <p><em>Figure 5: Effect of downsampling on image quality</em></p>
                  <p>For I.R. to RGB, I did not directly use PixelColor to generate color images but first used ImageGAN with wassterin loss. It ended up being blurry because we were averaging the loss over all of the pixels (Table - 1).</p>
                  <table>
                    <thead>
                      <tr>
                        <th>Input - IR</th>
                        <th>Target - RGB</th>
                        <th>Output - RGB</th>
                      </tr>
                    </thead>
                    <tbody>
                      <tr>
                        <td><img src="../assets/imgs/2021-01-22-15-41-01.png" alt="" /></td>
                        <td><img src="../assets/imgs/2021-01-22-15-41-24.png" alt="" /></td>
                        <td><img src="../assets/imgs/2021-01-22-15-41-32.png" alt="" /></td>
                      </tr>
                      <tr>
                        <td><img src="../assets/imgs/2021-01-22-15-42-40.png" alt="" /></td>
                        <td><img src="../assets/imgs/2021-01-22-15-42-49.png" alt="" /></td>
                        <td><img src="../assets/imgs/2021-01-22-15-42-57.png" alt="" /></td>
                      </tr>
                    </tbody>
                  </table>
                  <p><em>Table:1 Blurry Output when GAN’s are not used</em></p>
                  <p>An I.R. image is first passed through GAN, which generates grayscale output followed by PixColor for sampling RGB from the generated grayscale.</p>
                  <table>
                    <thead>
                      <tr>
                        <th>Input I.R.</th>
                        <th>Target GrayScale</th>
                        <th>Generated GrayScale</th>
                      </tr>
                    </thead>
                    <tbody>
                      <tr>
                        <td><img src="../assets/imgs/2021-01-24-19-26-49.png" alt="" /></td>
                        <td><img src="../assets/imgs/2021-01-24-19-27-04.png" alt="" /></td>
                        <td><img src="../assets/imgs/2021-01-24-19-27-15.png" alt="" /></td>
                      </tr>
                      <tr>
                        <td><img src="../assets/imgs/2021-01-24-19-28-14.png" alt="" /></td>
                        <td><img src="../assets/imgs/2021-01-24-19-28-30.png" alt="" /></td>
                        <td><img src="../assets/imgs/2021-01-24-19-28-46.png" alt="" /></td>
                      </tr>
                      <tr>
                        <td><img src="../assets/imgs/2021-01-24-19-29-55.png" alt="" /></td>
                        <td><img src="../assets/imgs/2021-01-24-19-30-02.png" alt="" /></td>
                        <td><img src="../assets/imgs/2021-01-24-19-30-08.png" alt="" /></td>
                      </tr>
                      <tr>
                        <td><img src="../assets/imgs/2021-01-24-19-44-27.png" alt="" /></td>
                        <td><img src="../assets/imgs/2021-01-24-19-44-42.png" alt="" /></td>
                        <td><img src="../assets/imgs/2021-01-24-19-44-49.png" alt="" /></td>
                      </tr>
                      <tr>
                        <td><img src="../assets/imgs/2021-01-24-19-45-59.png" alt="" /></td>
                        <td><img src="../assets/imgs/2021-01-24-19-46-12.png" alt="" /></td>
                        <td><img src="../assets/imgs/2021-01-24-19-46-25.png" alt="" /></td>
                      </tr>
                      <tr>
                        <td><img src="../assets/imgs/2021-01-24-19-48-05.png" alt="" /></td>
                        <td><img src="../assets/imgs/2021-01-24-19-48-17.png" alt="" /></td>
                        <td><img src="../assets/imgs/2021-01-24-19-48-30.png" alt="" /></td>
                      </tr>
                    </tbody>
                  </table>
                  <p><em>Table 2: Output of ImageGAN</em></p>
                  <p>The ImageGAN model produced close to ground truth for many situations (Table 2 first four rows). Still, the model’s performance deteriorated when uncommon objects were present in the I.R. image, like a person on the cycle or person themselves, highlighting the importance of richer data sources.</p>
                  <p>The last five images in carousal(at the top) are the results of the I.R. to color algorithm with key insights from each of the Images.</p>
                  <h2 id="datasets">Datasets</h2>
                  <p>The ADE20K  scene parsing dataset was used for PixColor training; it has 20K training and 1.5k validation samples. Also, there was a pretrained resnet101 network, which helped speed up the training process.
                    For I.R. to RGB translation, the kaist multispectral benchmark was used. It is divided into multiple files, with each file consisting of over 10000 images, but it lacks the spatial resolution. I tested six different datasets that have pair-wise I.R. and RGB images and found it to be most aligned.</p>
                  <h2 id="training-and-analysis">Training and Analysis</h2>
                  <p>PixColor was trained for 50 epochs, in 100% teacher-training mechanism, i.e. during training pixelcnn autoregressive considers ground truth samples as previous t inputs. The ImageGAN was trained for 150 epochs, I stopped after 150 epochs due to computation constraints. Evaluating generative models is very hard, that’s why I tested the color distribution generated by the PixColor and observed biases towards the brown color which was present in the ADE20k data itself.</p>
                  <p><img src="../assets/imgs/2021-01-24-21-53-44.png" alt="" /></p>
                  <p>Currently, I am studying more about GANs to train them effectively with fewer data points, if you have any questions or suggestion, please let me know on <a href="https://twitter.com/itskdme">Twitter</a> or <a href="https://www.instagram.com/itskd.me/">instagram</a>.</p>
                  <h3 id="references">References</h3>
                  <ul>
                    <li>Guadarrama, Sergio, et al. “Pixcolor: Pixel recursive colorization.” arXiv preprint arXiv:1705.07208 (2017).</li>
                    <li>Salimans, Tim, et al. “Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications.” arXiv preprint arXiv:1701.05517 (2017).</li>
                    <li>Oord, Aaron van den, et al. “Conditional image generation with pixelcnn decoders.” arXiv preprint arXiv:1606.05328 (2016).</li>
                    <li>Zhou, Bolei, et al. “Scene parsing through ade20k dataset.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.</li>
                    <li>Hwang, Soonmin, et al. “Multispectral pedestrian detection: Benchmark dataset and baseline.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.</li>
                    <li>Wang, Yuxuan, et al. “Tacotron: Towards end-to-end speech synthesis.” arXiv preprint arXiv:1703.10135 (2017).</li>
                    <li>Arjovsky, Martin, Soumith Chintala, and Léon Bottou. “Wasserstein generative adversarial networks.” International conference on machine learning. PMLR, 2017.</li>
                    <li>Isola, Phillip, et al. “Image-to-image translation with conditional adversarial networks.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.</li>
                  </ul>
                </div>
                <!-- Add backlinks to the current page -->
                <div class="related show" id="jekyll-seamless-relatedposts">
                  <h4 class="medium-small">Related Posts</h4>
                  <div class="related-wrapper">
                    <div class="related-group"><a href="/notes/basics_of_probability">
                        <p class="related-title">Points to study</p>
                        <p class="related-excerpt">
                          Machine Learning
                          Lecture 1
                          Hypothesis Testing
                          Density Estimation
                          Multi Instance Lear...</p>
                      </a></div>
                    <div class="related-group"><a href="/notes/basics_of_probability">
                        <p class="related-title">Basics of Probability</p>
                        <p class="related-excerpt">Probability
                          Measure of a subset under relation pretex
                          Basic Terms
                          Sample Space, Event, Proper Subset, Disjoint, Event Space,
                          Event s...</p>
                      </a></div>
                    <div class="related-group"><a href="/notes/tree_metrics_in_detail">
                        <p class="related-title">How does tree metrics works</p>
                        <p class="related-excerpt">Comparing MLTED, RF distance, and TreeVec
                          MLTED
                          Tree 1
                          Tree 2
                          ...</p>
                      </a></div>
                    <div class="related-group"><a href="/notes/basics_of_clonal_evolution">
                        <p class="related-title">Basics of Phylogeny</p>
                        <p class="related-excerpt">Phylogeny
                          Given a VAFs, the task is to generate ancestoral tree such that it adheres to the principles of perfect phylogeny.
                          Terminolog...</p>
                      </a></div>
                    <div class="related-group"><a href="/notes/xAI/intro">
                        <p class="related-title">Introduction to Explainable AI</p>
                        <p class="related-excerpt">Paper - 1, Measuring the Quality of Explanations: The System Causability Scale
                          With the current advancements in Machine Learning, and...</p>
                      </a></div>
                  </div>
                  <br/>
                </div>
              </main>
              <div id="copyright">
                <p id="copyright-notice">© Karan Dhingra. 2022</p>
              </div>
            </div>
          </div>
          <!-- Load KaTeX --></body>
      </html>